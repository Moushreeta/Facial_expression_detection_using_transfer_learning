{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "50be6a58-f1c2-4fd6-99a5-45b85df873b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f30a57e2-336d-40fd-86cb-fd6bb2545598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "89da6c1d-8feb-4da7-90fb-0545c76c5db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fec0fa24-ba19-4721-be87-c04421ade4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fer2013=\"C:/RESEARCH/FER2013NEW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fec625-6056-44f3-a1f5-ba3968c67eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ea9bf030-6f3d-4be7-a7db-0bf04f1db89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#import os\\n#import cv2\\n#import numpy as np\\n#from sklearn.model_selection import train_test_split\\n#from tensorflow.keras.utils import to_categorical\\n\\n# Define the paths to your dataset\\n#data_directory = \"C:/RESEARCH/FER2013NEW/train\"\\n#emotion_labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\\n#num_classes = len(emotion_labels)\\n\\n# Load and preprocess data\\n#X_data = []\\n#y_labels = []\\n\\n#for emotion_label in emotion_labels:\\n#    emotion_directory = os.path.join(data_directory, emotion_label)\\n    \\n #   for filename in os.listdir(emotion_directory):\\n  #      img_path = os.path.join(emotion_directory, filename)\\n  #      img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Read the image in grayscale\\n        \\n        # Resize the image to a consistent size (e.g., 48x48)\\n        img = cv2.resize(img, (48, 48))\\n        \\n        X_data.append(img)\\n        y_labels.append(emotion_labels.index(emotion_label))\\n\\nX_data = np.array(X_data)\\ny_labels = np.array(y_labels)\\n\\n# Normalize pixel values to [0, 1]\\n#X_data = X_data / 255.0\\n\\n# Convert labels to one-hot encoded format\\n#y_labels = to_categorical(y_labels, num_classes=num_classes)\\n\\n# Perform train-test split\\n#X_train, X_test, y_train, y_test = train_test_split(X_data, y_labels, test_size=0.2, random_state=42)\\n'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#import os\n",
    "#import cv2\n",
    "#import numpy as np\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Define the paths to your dataset\n",
    "#data_directory = \"C:/RESEARCH/FER2013NEW/train\"\n",
    "#emotion_labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
    "#num_classes = len(emotion_labels)\n",
    "\n",
    "# Load and preprocess data\n",
    "#X_data = []\n",
    "#y_labels = []\n",
    "\n",
    "#for emotion_label in emotion_labels:\n",
    "#    emotion_directory = os.path.join(data_directory, emotion_label)\n",
    "    \n",
    " #   for filename in os.listdir(emotion_directory):\n",
    "  #      img_path = os.path.join(emotion_directory, filename)\n",
    "  #      img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Read the image in grayscale\n",
    "        \n",
    "        # Resize the image to a consistent size (e.g., 48x48)\n",
    "        img = cv2.resize(img, (48, 48))\n",
    "        \n",
    "        X_data.append(img)\n",
    "        y_labels.append(emotion_labels.index(emotion_label))\n",
    "\n",
    "X_data = np.array(X_data)\n",
    "y_labels = np.array(y_labels)\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "#X_data = X_data / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "#y_labels = to_categorical(y_labels, num_classes=num_classes)\n",
    "\n",
    "# Perform train-test split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_data, y_labels, test_size=0.2, random_state=42)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c55b624f-8d0b-4b23-88cf-7e2ce2ef6576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport cv2\\nimport numpy as np\\nfrom sklearn.model_selection import train_test_split\\nfrom tensorflow.keras.utils import to_categorical\\n\\n# Define the paths to your dataset\\ndata_directory = \"C:/RESEARCH/FER2013NEW/test\"\\nemotion_labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\\nnum_classes = len(emotion_labels)\\n\\n# Load and preprocess data\\nX_data = []\\ny_labels = []\\n\\nfor emotion_label in emotion_labels:\\n    emotion_directory = os.path.join(data_directory, emotion_label)\\n    \\n    for filename in os.listdir(emotion_directory):\\n        img_path = os.path.join(emotion_directory, filename)\\n        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Read the image in grayscale\\n        \\n        # Resize the image to a consistent size (e.g., 48x48)\\n        img = cv2.resize(img, (48, 48))\\n        \\n        X_data.append(img)\\n        y_labels.append(emotion_labels.index(emotion_label))\\n\\nX_data = np.array(X_data)\\ny_labels = np.array(y_labels)\\n'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Define the paths to your dataset\n",
    "data_directory = \"C:/RESEARCH/FER2013NEW/test\"\n",
    "emotion_labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
    "num_classes = len(emotion_labels)\n",
    "\n",
    "# Load and preprocess data\n",
    "X_data = []\n",
    "y_labels = []\n",
    "\n",
    "for emotion_label in emotion_labels:\n",
    "    emotion_directory = os.path.join(data_directory, emotion_label)\n",
    "    \n",
    "    for filename in os.listdir(emotion_directory):\n",
    "        img_path = os.path.join(emotion_directory, filename)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Read the image in grayscale\n",
    "        \n",
    "        # Resize the image to a consistent size (e.g., 48x48)\n",
    "        img = cv2.resize(img, (48, 48))\n",
    "        \n",
    "        X_data.append(img)\n",
    "        y_labels.append(emotion_labels.index(emotion_label))\n",
    "\n",
    "X_data = np.array(X_data)\n",
    "y_labels = np.array(y_labels)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b3569830-48c1-47e4-a9f9-e0fca3f28344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import numpy as np\\nimport tensorflow as tf\\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\\nfrom tensorflow.keras.applications import VGG16\\nfrom tensorflow.keras.layers import Dense, Flatten\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import classification_report, confusion_matrix\\nimport matplotlib.pyplot as plt\\n\\n\\n\\n# Perform train-test split\\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_labels, test_size=0.2, random_state=42)\\n\\n# Preprocess images\\nX_train = X_train / 255.0\\nX_test = X_test / 255.0\\nX_train = X_train.reshape(X_train.shape[0], 48, 48,3)\\nX_test = X_test.reshape(X_test.shape[0], 48, 48,3)\\n\\n# Data augmentation\\ndata_augmentation = ImageDataGenerator(rotation_range=15, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\\n\\n# Load pre-trained VGG16 model\\nbase_model = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48))\\n\\n# Freeze layers\\nfor layer in base_model.layers:\\n    layer.trainable = False\\n\\n# Build your own classification layers\\nmodel = tf.keras.Sequential([\\n    base_model,\\n    Flatten(),\\n    Dense(128, activation='relu'),\\n    Dense(7, activation='softmax')  # 7 emotions\\n])\\n\\n# Compile the model\\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\\n\\n# Train the model\\nbatch_size = 32\\nepochs = 10\\n\\nhistory = model.fit(data_augmentation.flow(X_train, y_train, batch_size=batch_size),\\n                    steps_per_epoch=len(X_train) // batch_size,\\n                    epochs=epochs,\\n                    validation_data=(X_test, y_test))\\n\\n# Evaluate the model\\ny_pred = model.predict(X_test)\\ny_pred_classes = np.argmax(y_pred, axis=1)\\ny_true = np.argmax(y_test, axis=1)\\n\\nprint(classification_report(y_true, y_pred_classes))\\nconf_matrix = confusion_matrix(y_true, y_pred_classes)\\nprint(conf_matrix)\\n\\n# Plot accuracy and loss curves\\nplt.figure(figsize=(12, 4))\\nplt.subplot(1, 2, 1)\\nplt.plot(history.history['accuracy'], label='accuracy')\\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\\nplt.legend()\\nplt.title('Accuracy')\\n\\nplt.subplot(1, 2, 2)\\nplt.plot(history.history['loss'], label='loss')\\nplt.plot(history.history['val_loss'], label='val_loss')\\nplt.legend()\\nplt.title('Loss')\\n\\nplt.show()\\n\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess images\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_train = X_train.reshape(X_train.shape[0], 48, 48,3)\n",
    "X_test = X_test.reshape(X_test.shape[0], 48, 48,3)\n",
    "\n",
    "# Data augmentation\n",
    "data_augmentation = ImageDataGenerator(rotation_range=15, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "\n",
    "# Load pre-trained VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48))\n",
    "\n",
    "# Freeze layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Build your own classification layers\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(7, activation='softmax')  # 7 emotions\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(data_augmentation.flow(X_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=len(X_train) // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(classification_report(y_true, y_pred_classes))\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "print(conf_matrix)\n",
    "\n",
    "# Plot accuracy and loss curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "046728f6-63a4-47f6-ba07-ab6872693e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b4410bce-a678-4db5-aae9-a7a34fc8d31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder=\"C:/RESEARCH/FER2013NEW/train\"\n",
    "test_folder=\"C:/RESEARCH/FER2013NEW/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "268c92d8-24e1-4ce3-ac31-335547093895",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]  # List of emotion labels\n",
    "num_classes = len(emotion_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4114d5a7-4caa-4ab6-ba68-59dc661934cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5a6cb97e-e43a-45dc-adc1-97f821c7a20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []  # Create an empty list to store images\n",
    "    labels = []  # Create an empty list to store labels\n",
    "    for label_idx, emotion_label in enumerate(emotion_labels):\n",
    "        # Construct the path to the folder containing images for the current emotion\n",
    "        emotion_folder = os.path.join(folder, emotion_label)\n",
    "        for filename in os.listdir(emotion_folder):\n",
    "            # Construct the path to each image and load it using OpenCV\n",
    "            img_path = os.path.join(emotion_folder, filename)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, (48, 48))  # Resize the image to a fixed size\n",
    "            img_rgb = np.stack((img,) * 3, axis=-1)\n",
    "            images.append(img_rgb)\n",
    "            labels.append(label_idx)  # Add the corresponding label index to the list\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "440e59d0-e9fd-42fd-96d9-ca06ddb292f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data=\"C:/RESEARCH/FER2013NEW/train/Training_50580\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7f2ce228-e46f-4450-807e-0eae5cd05f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=cv2.imread(\"C:/RESEARCH/FER2013NEW/train/happy/Training_50580.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1b63d67a-945d-4748-b57f-f7eda13c092e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d43f5b6c40>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3f9f8fde-7c96-4246-8888-2dbf2fc6e678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48, 3)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6d018131-d709-4aab-85e2-129ed488a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Preprocess Data\n",
    "X_train, y_train = load_images_from_folder(train_folder)  # Load and preprocess training data\n",
    "X_test, y_test = load_images_from_folder(test_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b8744d45-c569-46b4-a519-14674c57e060",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)  # Convert the list of images to a NumPy array\n",
    "y_train = np.array(y_train)  # Convert the list of labels to a NumPy array\n",
    "X_test = np.array(X_test)  # Convert the list of images to a NumPy array\n",
    "y_test = np.array(y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b4350897-a768-4cd2-9796-a8ab1cc4e700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709, 48, 48, 3)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "0d94a8c7-38f2-48df-9b3d-cfeeefb9d174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709,)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c693c0fe-5275-498b-9b46-f72fc030c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3dc19364-0b9a-4ecd-808f-73320d354339",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat=to_categorical(y_train,num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b3ba16dc-8b9e-49e5-a2d4-b1f7a217a65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f7da9f34-fa3a-43da-bd0c-77cae5d1c6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_cat=to_categorical(y_test,num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "19e7d28d-4db2-4471-8046-0a3a728ac51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "c36abc82-7a6e-4dd5-88c2-f023f5c2b9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709, 7)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "da3243ea-d88d-497c-9f53-c20bfcb4ea40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7178, 7)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "eb86ba4b-5dfa-49bc-8546-ce1b15268253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6964a000-0709-4e0f-8d2c-5bb7587d69b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train/255\n",
    "X_test=X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "34d6a4fb-dcb1-42bf-ba54-0e8d9a803d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8c7bade5-a66d-48a8-8c74-c307c88fbed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer learning\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7e95d5ea-451e-470e-b038-984c579da0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom classification layers on top of the pre-trained base\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)base_model = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5fbcc27c-c152-4003-9748-74d2f980b02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6d9e0fcf-dc4b-46e2-8cc8-4d26cbb2035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze layers in the pre-trained base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a9381bdf-67d0-4c4e-87f3-040382bb0d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "49c637d6-c849-4962-ae95-cb3dec72762a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "898/898 [==============================] - 763s 846ms/step - loss: 1.6169 - accuracy: 0.3646 - val_loss: 1.5609 - val_accuracy: 0.3969\n",
      "Epoch 2/10\n",
      "898/898 [==============================] - 646s 719ms/step - loss: 1.5324 - accuracy: 0.4088 - val_loss: 1.5271 - val_accuracy: 0.4028\n",
      "Epoch 3/10\n",
      "898/898 [==============================] - 642s 715ms/step - loss: 1.4940 - accuracy: 0.4266 - val_loss: 1.5247 - val_accuracy: 0.4115\n",
      "Epoch 4/10\n",
      "898/898 [==============================] - 655s 730ms/step - loss: 1.4629 - accuracy: 0.4400 - val_loss: 1.4825 - val_accuracy: 0.4266\n",
      "Epoch 5/10\n",
      "898/898 [==============================] - 655s 730ms/step - loss: 1.4310 - accuracy: 0.4530 - val_loss: 1.4863 - val_accuracy: 0.4207\n",
      "Epoch 6/10\n",
      "898/898 [==============================] - 644s 717ms/step - loss: 1.4059 - accuracy: 0.4642 - val_loss: 1.4814 - val_accuracy: 0.4238\n",
      "Epoch 7/10\n",
      "898/898 [==============================] - 632s 704ms/step - loss: 1.3761 - accuracy: 0.4802 - val_loss: 1.4906 - val_accuracy: 0.4259\n",
      "Epoch 8/10\n",
      "898/898 [==============================] - 662s 737ms/step - loss: 1.3490 - accuracy: 0.4900 - val_loss: 1.4847 - val_accuracy: 0.4291\n",
      "Epoch 9/10\n",
      "898/898 [==============================] - 743s 827ms/step - loss: 1.3253 - accuracy: 0.5018 - val_loss: 1.4731 - val_accuracy: 0.4368\n",
      "Epoch 10/10\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.2976 - accuracy: 0.5121"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load pre-trained VGG16 model (excluding top layers)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
    "\n",
    "# Add custom classification layers on top of the pre-trained base\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze layers in the pre-trained base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(X_train, y_train_cat, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test_cat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6df0a887-0653-480e-a274-d486d2c93d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 146s 648ms/step - loss: 1.4712 - accuracy: 0.4351\n",
      "Test loss: 1.4712\n",
      "Test accuracy: 0.4351\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test_cat)\n",
    "print(f\"Test loss: {loss:.4f}\")\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "7b2c94d3-02aa-41a3-8d9b-cec79d432e05",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [182]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot accuracy and loss curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716e99a7-b89e-4f25-bcbf-1603c93d5f23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
